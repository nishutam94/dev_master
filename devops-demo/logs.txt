
==> Audit <==
|-----------|-----------------------|----------|------|---------|---------------------|---------------------|
|  Command  |         Args          | Profile  | User | Version |     Start Time      |      End Time       |
|-----------|-----------------------|----------|------|---------|---------------------|---------------------|
| delete    |                       | minikube | root | v1.36.0 | 10 Jul 25 06:34 EDT | 10 Jul 25 06:34 EDT |
| start     | --driver=docker       | minikube | root | v1.36.0 | 10 Jul 25 06:34 EDT |                     |
| start     | --driver=docker       | minikube | root | v1.36.0 | 10 Jul 25 06:34 EDT |                     |
| start     | --driver=none         | minikube | root | v1.36.0 | 10 Jul 25 06:34 EDT |                     |
| start     | --driver=none         | minikube | root | v1.36.0 | 10 Jul 25 06:50 EDT | 10 Jul 25 06:50 EDT |
| start     | --driver=none         | minikube | root | v1.36.0 | 10 Jul 25 06:50 EDT | 10 Jul 25 06:51 EDT |
| ip        |                       | minikube | root | v1.36.0 | 10 Jul 25 14:39 EDT | 10 Jul 25 14:39 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 10 Jul 25 14:50 EDT | 10 Jul 25 14:51 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 10 Jul 25 14:53 EDT | 10 Jul 25 14:54 EDT |
| dashboard |                       | minikube | root | v1.36.0 | 10 Jul 25 14:54 EDT |                     |
| delete    |                       | minikube | root | v1.36.0 | 10 Jul 25 14:54 EDT | 10 Jul 25 14:54 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 10 Jul 25 14:55 EDT |                     |
| delete    |                       | minikube | root | v1.36.0 | 10 Jul 25 14:59 EDT | 10 Jul 25 14:59 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 10 Jul 25 15:01 EDT |                     |
| delete    |                       | minikube | root | v1.36.0 | 10 Jul 25 15:02 EDT | 10 Jul 25 15:02 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 10 Jul 25 15:02 EDT |                     |
| delete    |                       | minikube | root | v1.36.0 | 10 Jul 25 15:03 EDT | 10 Jul 25 15:03 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 10 Jul 25 15:03 EDT |                     |
| start     | --driver=none --force | minikube | root | v1.36.0 | 10 Jul 25 15:11 EDT | 10 Jul 25 15:12 EDT |
| image     | load stress-ng-app    | minikube | root | v1.36.0 | 11 Jul 25 08:23 EDT | 11 Jul 25 08:23 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 11 Jul 25 08:23 EDT | 11 Jul 25 08:24 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 11 Jul 25 08:25 EDT | 11 Jul 25 08:25 EDT |
| delete    |                       | minikube | root | v1.36.0 | 11 Jul 25 08:27 EDT | 11 Jul 25 08:27 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 11 Jul 25 08:27 EDT |                     |
| delete    |                       | minikube | root | v1.36.0 | 11 Jul 25 08:28 EDT | 11 Jul 25 08:28 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 11 Jul 25 08:28 EDT |                     |
| delete    |                       | minikube | root | v1.36.0 | 11 Jul 25 11:29 EDT | 11 Jul 25 11:29 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 11 Jul 25 11:29 EDT |                     |
| delete    |                       | minikube | root | v1.36.0 | 11 Jul 25 11:32 EDT | 11 Jul 25 11:32 EDT |
| start     | --driver=none --force | minikube | root | v1.36.0 | 11 Jul 25 11:32 EDT |                     |
|-----------|-----------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/07/11 11:32:28
Running on machine: SR48DML033S0202
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0711 11:32:28.215016 1275909 out.go:345] Setting OutFile to fd 1 ...
I0711 11:32:28.215087 1275909 out.go:397] isatty.IsTerminal(1) = true
I0711 11:32:28.215089 1275909 out.go:358] Setting ErrFile to fd 2...
I0711 11:32:28.215091 1275909 out.go:397] isatty.IsTerminal(2) = true
I0711 11:32:28.215192 1275909 root.go:338] Updating PATH: /root/.minikube/bin
I0711 11:32:28.215439 1275909 out.go:352] Setting JSON to false
I0711 11:32:28.219139 1275909 start.go:130] hostinfo: {"hostname":"SR48DML033S0202","uptime":5461295,"bootTime":1746786653,"procs":1302,"os":"linux","platform":"centos","platformFamily":"rhel","platformVersion":"9","kernelVersion":"6.2.0-emr.bkc.6.2.19.2.52.x86_64","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"88888888-8887-0c18-0706-0722a5a5a5a5"}
I0711 11:32:28.219173 1275909 start.go:140] virtualization: kvm host
I0711 11:32:28.219804 1275909 out.go:177] üòÑ  minikube v1.36.0 on Centos 9
I0711 11:32:28.220219 1275909 out.go:177]     ‚ñ™ KUBECONFIG=/etc/kubernetes/admin.conf
I0711 11:32:28.220314 1275909 notify.go:220] Checking for updates...
W0711 11:32:28.220330 1275909 preload.go:293] Failed to list preload files: open /root/.minikube/cache/preloaded-tarball: no such file or directory
W0711 11:32:28.220819 1275909 out.go:270] ‚ùó  minikube skips various validations when --force is supplied; this may lead to unexpected behavior
I0711 11:32:28.220936 1275909 driver.go:404] Setting default libvirt URI to qemu:///system
I0711 11:32:28.221751 1275909 out.go:177] ‚ú®  Using the none driver based on user configuration
I0711 11:32:28.222078 1275909 start.go:304] selected driver: none
I0711 11:32:28.222081 1275909 start.go:908] validating driver "none" against <nil>
I0711 11:32:28.222087 1275909 start.go:919] status for none: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0711 11:32:28.224514 1275909 start_flags.go:311] no existing cluster config was found, will generate one from the flags 
I0711 11:32:28.229021 1275909 start_flags.go:394] Using suggested 6000MB memory alloc based on sys=128005MB, container=0MB
I0711 11:32:28.229102 1275909 start_flags.go:958] Wait components to verify : map[apiserver:true system_pods:true]
I0711 11:32:28.229113 1275909 cni.go:84] Creating CNI manager for ""
I0711 11:32:28.229144 1275909 cni.go:158] "none" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0711 11:32:28.229148 1275909 start_flags.go:320] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0711 11:32:28.229182 1275909 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:6000 CPUs:2 DiskSize:20000 Driver:none HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:false EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0711 11:32:28.230188 1275909 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0711 11:32:28.230986 1275909 profile.go:143] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0711 11:32:28.231002 1275909 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/config.json: {Name:mk270d1b5db5965f2dc9e9e25770a63417031943 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0711 11:32:28.231070 1275909 start.go:360] acquireMachinesLock for minikube: {Name:mkc8ab01ad3ea83211c505c81a7ee49a8e3ecb89 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0711 11:32:28.231080 1275909 start.go:364] duration metric: took 6.095¬µs to acquireMachinesLock for "minikube"
I0711 11:32:28.231085 1275909 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:6000 CPUs:2 DiskSize:20000 Driver:none HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:false EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0711 11:32:28.231106 1275909 start.go:125] createHost starting for "" (driver="none")
I0711 11:32:28.231747 1275909 out.go:177] ü§π  Running on localhost (CPUs=128, Memory=128005MB, Disk=71616MB) ...
I0711 11:32:28.232382 1275909 exec_runner.go:51] Run: systemctl --version
I0711 11:32:28.235559 1275909 start.go:159] libmachine.API.Create for "minikube" (driver="none")
I0711 11:32:28.235570 1275909 client.go:168] LocalClient.Create starting
I0711 11:32:28.235590 1275909 main.go:141] libmachine: Reading certificate data from /root/.minikube/certs/ca.pem
I0711 11:32:28.235602 1275909 main.go:141] libmachine: Decoding PEM data...
I0711 11:32:28.235609 1275909 main.go:141] libmachine: Parsing certificate...
I0711 11:32:28.235640 1275909 main.go:141] libmachine: Reading certificate data from /root/.minikube/certs/cert.pem
I0711 11:32:28.235647 1275909 main.go:141] libmachine: Decoding PEM data...
I0711 11:32:28.235652 1275909 main.go:141] libmachine: Parsing certificate...
I0711 11:32:28.235821 1275909 client.go:171] duration metric: took 248.16¬µs to LocalClient.Create
I0711 11:32:28.235828 1275909 start.go:167] duration metric: took 269.218¬µs to libmachine.API.Create "minikube"
I0711 11:32:28.235831 1275909 start.go:293] postStartSetup for "minikube" (driver="none")
I0711 11:32:28.235859 1275909 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0711 11:32:28.235875 1275909 exec_runner.go:51] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0711 11:32:28.245496 1275909 main.go:141] libmachine: Couldn't set key PLATFORM_ID, no corresponding struct field found
I0711 11:32:28.245507 1275909 main.go:141] libmachine: Couldn't set key LOGO, no corresponding struct field found
I0711 11:32:28.245513 1275909 main.go:141] libmachine: Couldn't set key CPE_NAME, no corresponding struct field found
I0711 11:32:28.245518 1275909 main.go:141] libmachine: Couldn't set key REDHAT_SUPPORT_PRODUCT, no corresponding struct field found
I0711 11:32:28.245523 1275909 main.go:141] libmachine: Couldn't set key REDHAT_SUPPORT_PRODUCT_VERSION, no corresponding struct field found
I0711 11:32:28.246287 1275909 out.go:177] ‚ÑπÔ∏è  OS release is CentOS Stream 9
I0711 11:32:28.246906 1275909 filesync.go:126] Scanning /root/.minikube/addons for local assets ...
I0711 11:32:28.246924 1275909 filesync.go:126] Scanning /root/.minikube/files for local assets ...
I0711 11:32:28.246931 1275909 start.go:296] duration metric: took 11.098663ms for postStartSetup
I0711 11:32:28.247471 1275909 profile.go:143] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0711 11:32:28.247523 1275909 start.go:128] duration metric: took 16.413283ms to createHost
I0711 11:32:28.247526 1275909 start.go:83] releasing machines lock for "minikube", held for 16.442901ms
I0711 11:32:28.248403 1275909 out.go:177] üåê  Found network options:
I0711 11:32:28.249046 1275909 out.go:177]     ‚ñ™ HTTP_PROXY=http://proxy.ch.intel.com:911
W0711 11:32:28.249654 1275909 out.go:270] ‚ùó  You appear to be using a proxy, but your NO_PROXY environment does not include the minikube IP (10.190.181.134).
I0711 11:32:28.250126 1275909 out.go:177] üìò  Please see https://minikube.sigs.k8s.io/docs/handbook/vpn_and_proxy/ for more details
I0711 11:32:28.250767 1275909 out.go:177]     ‚ñ™ HTTPS_PROXY=http://proxy.ch.intel.com:912
I0711 11:32:28.251434 1275909 out.go:177]     ‚ñ™ NO_PROXY=.intel.com,10.0.0.0/8,10.190.181.134,192.168.0.0/16,localhost,127.0.0.0/8
I0711 11:32:28.252084 1275909 exec_runner.go:51] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0711 11:32:28.252157 1275909 exec_runner.go:51] Run: curl -sS -m 2 https://registry.k8s.io/
W0711 11:32:28.253916 1275909 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0711 11:32:28.253951 1275909 exec_runner.go:51] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0711 11:32:28.263755 1275909 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0711 11:32:28.263763 1275909 start.go:495] detecting cgroup driver to use...
I0711 11:32:28.263786 1275909 detect.go:190] detected "systemd" cgroup driver on host os
I0711 11:32:28.263837 1275909 exec_runner.go:51] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0711 11:32:28.284242 1275909 exec_runner.go:51] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0711 11:32:28.294783 1275909 exec_runner.go:51] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0711 11:32:28.305068 1275909 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0711 11:32:28.305110 1275909 exec_runner.go:51] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0711 11:32:28.314903 1275909 exec_runner.go:51] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0711 11:32:28.325006 1275909 exec_runner.go:51] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0711 11:32:28.334764 1275909 exec_runner.go:51] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0711 11:32:28.344812 1275909 exec_runner.go:51] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0711 11:32:28.354256 1275909 exec_runner.go:51] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0711 11:32:28.363606 1275909 exec_runner.go:51] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0711 11:32:28.373691 1275909 exec_runner.go:51] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0711 11:32:28.383504 1275909 exec_runner.go:51] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0711 11:32:28.391738 1275909 exec_runner.go:51] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0711 11:32:28.400471 1275909 exec_runner.go:51] Run: sudo systemctl daemon-reload
I0711 11:32:28.635405 1275909 exec_runner.go:51] Run: sudo systemctl restart containerd
I0711 11:32:28.711551 1275909 start.go:495] detecting cgroup driver to use...
I0711 11:32:28.711569 1275909 detect.go:190] detected "systemd" cgroup driver on host os
I0711 11:32:28.711621 1275909 exec_runner.go:51] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0711 11:32:28.731890 1275909 exec_runner.go:51] Run: which cri-dockerd
I0711 11:32:28.732536 1275909 exec_runner.go:51] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0711 11:32:28.741773 1275909 exec_runner.go:144] found /etc/systemd/system/cri-docker.service.d/10-cni.conf, removing ...
I0711 11:32:28.741790 1275909 exec_runner.go:203] rm: /etc/systemd/system/cri-docker.service.d/10-cni.conf
I0711 11:32:28.741816 1275909 exec_runner.go:151] cp: memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (196 bytes)
I0711 11:32:28.741891 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube2646408772 /etc/systemd/system/cri-docker.service.d/10-cni.conf
I0711 11:32:28.750921 1275909 exec_runner.go:51] Run: sudo systemctl unmask docker.service
I0711 11:32:28.987786 1275909 exec_runner.go:51] Run: sudo systemctl enable docker.socket
I0711 11:32:29.217673 1275909 docker.go:587] configuring docker to use "systemd" as cgroup driver...
I0711 11:32:29.217722 1275909 exec_runner.go:144] found /etc/docker/daemon.json, removing ...
I0711 11:32:29.217725 1275909 exec_runner.go:203] rm: /etc/docker/daemon.json
I0711 11:32:29.217748 1275909 exec_runner.go:151] cp: memory --> /etc/docker/daemon.json (129 bytes)
I0711 11:32:29.217984 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube804430303 /etc/docker/daemon.json
I0711 11:32:29.227156 1275909 exec_runner.go:51] Run: sudo systemctl reset-failed docker
I0711 11:32:29.238563 1275909 exec_runner.go:51] Run: sudo systemctl daemon-reload
I0711 11:32:29.471420 1275909 exec_runner.go:51] Run: sudo systemctl restart docker
I0711 11:32:29.673532 1275909 exec_runner.go:84] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.421339548s)
I0711 11:32:30.044206 1275909 exec_runner.go:51] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0711 11:32:30.056313 1275909 exec_runner.go:51] Run: sudo systemctl stop cri-docker.socket
I0711 11:32:30.071815 1275909 exec_runner.go:51] Run: sudo systemctl is-active --quiet service cri-docker.service
I0711 11:32:30.083508 1275909 exec_runner.go:51] Run: sudo systemctl unmask cri-docker.socket
I0711 11:32:30.312095 1275909 exec_runner.go:51] Run: sudo systemctl enable cri-docker.socket
I0711 11:32:30.548063 1275909 exec_runner.go:51] Run: sudo systemctl daemon-reload
I0711 11:32:30.776938 1275909 exec_runner.go:51] Run: sudo systemctl restart cri-docker.socket
I0711 11:32:30.798754 1275909 exec_runner.go:51] Run: sudo systemctl reset-failed cri-docker.service
I0711 11:32:30.810565 1275909 exec_runner.go:51] Run: sudo systemctl daemon-reload
I0711 11:32:31.038944 1275909 exec_runner.go:51] Run: sudo systemctl restart cri-docker.service
I0711 11:32:31.104043 1275909 exec_runner.go:51] Run: sudo systemctl is-active --quiet service cri-docker.service
I0711 11:32:31.116248 1275909 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0711 11:32:31.116276 1275909 exec_runner.go:51] Run: stat /var/run/cri-dockerd.sock
I0711 11:32:31.117176 1275909 start.go:563] Will wait 60s for crictl version
I0711 11:32:31.117201 1275909 exec_runner.go:51] Run: which crictl
I0711 11:32:31.117665 1275909 exec_runner.go:51] Run: sudo /usr/local/bin/crictl version
I0711 11:32:31.142089 1275909 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.3.2
RuntimeApiVersion:  v1
I0711 11:32:31.142115 1275909 exec_runner.go:51] Run: docker version --format {{.Server.Version}}
I0711 11:32:31.157144 1275909 exec_runner.go:51] Run: docker version --format {{.Server.Version}}
I0711 11:32:31.172353 1275909 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.3.2 ...
I0711 11:32:31.174542 1275909 out.go:177]     ‚ñ™ env HTTP_PROXY=http://proxy.ch.intel.com:911
I0711 11:32:31.176377 1275909 out.go:177]     ‚ñ™ env HTTPS_PROXY=http://proxy.ch.intel.com:912
I0711 11:32:31.178179 1275909 out.go:177]     ‚ñ™ env NO_PROXY=.intel.com,10.0.0.0/8,10.190.181.134,192.168.0.0/16,localhost,127.0.0.0/8
I0711 11:32:31.179752 1275909 exec_runner.go:51] Run: grep 127.0.0.1	host.minikube.internal$ /etc/hosts
I0711 11:32:31.180760 1275909 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:6000 CPUs:2 DiskSize:20000 Driver:none HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:false EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:10.190.181.134 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0711 11:32:31.180806 1275909 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0711 11:32:31.180810 1275909 kubeadm.go:926] updating node { 10.190.181.134 8443 v1.33.1 docker true true} ...
I0711 11:32:31.180849 1275909 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=SR48DML033S0202 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=10.190.181.134

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:false EnableDefaultCNI:false CNI:}
I0711 11:32:31.180867 1275909 exec_runner.go:51] Run: docker info --format {{.CgroupDriver}}
I0711 11:32:31.220909 1275909 cni.go:84] Creating CNI manager for ""
I0711 11:32:31.220928 1275909 cni.go:158] "none" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0711 11:32:31.220934 1275909 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0711 11:32:31.220946 1275909 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:10.190.181.134 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:SR48DML033S0202 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "10.190.181.134"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:10.190.181.134 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0711 11:32:31.221003 1275909 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.190.181.134
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "SR48DML033S0202"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "10.190.181.134"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "10.190.181.134"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0711 11:32:31.221032 1275909 exec_runner.go:51] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0711 11:32:31.230057 1275909 binaries.go:47] Didn't find k8s binaries: sudo ls /var/lib/minikube/binaries/v1.33.1: exit status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/binaries/v1.33.1': No such file or directory

Initiating transfer...
I0711 11:32:31.230084 1275909 exec_runner.go:51] Run: sudo mkdir -p /var/lib/minikube/binaries/v1.33.1
I0711 11:32:31.238378 1275909 binary.go:74] Not caching binary, using https://dl.k8s.io/release/v1.33.1/bin/linux/amd64/kubectl?checksum=file:https://dl.k8s.io/release/v1.33.1/bin/linux/amd64/kubectl.sha256
I0711 11:32:31.238399 1275909 exec_runner.go:151] cp: /root/.minikube/cache/linux/amd64/v1.33.1/kubectl --> /var/lib/minikube/binaries/v1.33.1/kubectl (60121272 bytes)
I0711 11:32:31.238638 1275909 binary.go:74] Not caching binary, using https://dl.k8s.io/release/v1.33.1/bin/linux/amd64/kubelet?checksum=file:https://dl.k8s.io/release/v1.33.1/bin/linux/amd64/kubelet.sha256
I0711 11:32:31.238661 1275909 exec_runner.go:51] Run: sudo systemctl is-active --quiet service kubelet
I0711 11:32:31.238748 1275909 binary.go:74] Not caching binary, using https://dl.k8s.io/release/v1.33.1/bin/linux/amd64/kubeadm?checksum=file:https://dl.k8s.io/release/v1.33.1/bin/linux/amd64/kubeadm.sha256
I0711 11:32:31.238771 1275909 exec_runner.go:151] cp: /root/.minikube/cache/linux/amd64/v1.33.1/kubeadm --> /var/lib/minikube/binaries/v1.33.1/kubeadm (74535096 bytes)
I0711 11:32:31.250439 1275909 exec_runner.go:151] cp: /root/.minikube/cache/linux/amd64/v1.33.1/kubelet --> /var/lib/minikube/binaries/v1.33.1/kubelet (81690916 bytes)
I0711 11:32:31.261019 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube3560744291 /var/lib/minikube/binaries/v1.33.1/kubectl
I0711 11:32:31.266404 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube2663703927 /var/lib/minikube/binaries/v1.33.1/kubeadm
I0711 11:32:31.280022 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube3564491078 /var/lib/minikube/binaries/v1.33.1/kubelet
I0711 11:32:31.690017 1275909 exec_runner.go:51] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0711 11:32:31.698787 1275909 exec_runner.go:144] found /etc/systemd/system/kubelet.service.d/10-kubeadm.conf, removing ...
I0711 11:32:31.698793 1275909 exec_runner.go:203] rm: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
I0711 11:32:31.698818 1275909 exec_runner.go:151] cp: memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (316 bytes)
I0711 11:32:31.698890 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube1577866267 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
I0711 11:32:31.708091 1275909 exec_runner.go:144] found /lib/systemd/system/kubelet.service, removing ...
I0711 11:32:31.708097 1275909 exec_runner.go:203] rm: /lib/systemd/system/kubelet.service
I0711 11:32:31.708120 1275909 exec_runner.go:151] cp: memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0711 11:32:31.708188 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube289086415 /lib/systemd/system/kubelet.service
I0711 11:32:31.717084 1275909 exec_runner.go:151] cp: memory --> /var/tmp/minikube/kubeadm.yaml.new (2298 bytes)
I0711 11:32:31.717153 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube1983712975 /var/tmp/minikube/kubeadm.yaml.new
I0711 11:32:31.725757 1275909 exec_runner.go:51] Run: grep 10.190.181.134	control-plane.minikube.internal$ /etc/hosts
I0711 11:32:31.726834 1275909 exec_runner.go:51] Run: sudo systemctl daemon-reload
I0711 11:32:31.958274 1275909 exec_runner.go:51] Run: sudo systemctl start kubelet
I0711 11:32:31.984001 1275909 certs.go:68] Setting up /root/.minikube/profiles/minikube for IP: 10.190.181.134
I0711 11:32:31.984006 1275909 certs.go:194] generating shared ca certs ...
I0711 11:32:31.984013 1275909 certs.go:226] acquiring lock for ca certs: {Name:mkb814c315fe9b7fabb439d6d58c5448fbb7853c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0711 11:32:31.984088 1275909 certs.go:235] skipping valid "minikubeCA" ca cert: /root/.minikube/ca.key
I0711 11:32:31.984106 1275909 certs.go:235] skipping valid "proxyClientCA" ca cert: /root/.minikube/proxy-client-ca.key
I0711 11:32:31.984109 1275909 certs.go:256] generating profile certs ...
I0711 11:32:31.984136 1275909 certs.go:363] generating signed profile cert for "minikube-user": /root/.minikube/profiles/minikube/client.key
I0711 11:32:31.984140 1275909 crypto.go:68] Generating cert /root/.minikube/profiles/minikube/client.crt with IP's: []
I0711 11:32:32.031438 1275909 crypto.go:156] Writing cert to /root/.minikube/profiles/minikube/client.crt ...
I0711 11:32:32.031443 1275909 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/client.crt: {Name:mk09878e812b07af637940656ec44996daba95aa Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0711 11:32:32.031496 1275909 crypto.go:164] Writing key to /root/.minikube/profiles/minikube/client.key ...
I0711 11:32:32.031498 1275909 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/client.key: {Name:mkf3b978f9858871583d8228f83a87a85b7d106f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0711 11:32:32.031523 1275909 certs.go:363] generating signed profile cert for "minikube": /root/.minikube/profiles/minikube/apiserver.key.13fa738f
I0711 11:32:32.031527 1275909 crypto.go:68] Generating cert /root/.minikube/profiles/minikube/apiserver.crt.13fa738f with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 10.190.181.134]
I0711 11:32:32.353674 1275909 crypto.go:156] Writing cert to /root/.minikube/profiles/minikube/apiserver.crt.13fa738f ...
I0711 11:32:32.353680 1275909 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/apiserver.crt.13fa738f: {Name:mke17513d62551dd5452546ab55c2677b8fb136b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0711 11:32:32.353733 1275909 crypto.go:164] Writing key to /root/.minikube/profiles/minikube/apiserver.key.13fa738f ...
I0711 11:32:32.353735 1275909 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/apiserver.key.13fa738f: {Name:mk8782aea7ac3b50cc415fcc2a220de7657fd009 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0711 11:32:32.353757 1275909 certs.go:381] copying /root/.minikube/profiles/minikube/apiserver.crt.13fa738f -> /root/.minikube/profiles/minikube/apiserver.crt
I0711 11:32:32.354079 1275909 certs.go:385] copying /root/.minikube/profiles/minikube/apiserver.key.13fa738f -> /root/.minikube/profiles/minikube/apiserver.key
I0711 11:32:32.354192 1275909 certs.go:363] generating signed profile cert for "aggregator": /root/.minikube/profiles/minikube/proxy-client.key
I0711 11:32:32.354197 1275909 crypto.go:68] Generating cert /root/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0711 11:32:32.493711 1275909 crypto.go:156] Writing cert to /root/.minikube/profiles/minikube/proxy-client.crt ...
I0711 11:32:32.493716 1275909 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/proxy-client.crt: {Name:mkcab3ddb18cd096d978df14d87a44e804896057 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0711 11:32:32.493766 1275909 crypto.go:164] Writing key to /root/.minikube/profiles/minikube/proxy-client.key ...
I0711 11:32:32.493768 1275909 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/proxy-client.key: {Name:mkaff5bf6f623f02423597918f5f33c2a99a3db1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0711 11:32:32.493842 1275909 certs.go:484] found cert: /root/.minikube/certs/ca-key.pem (1679 bytes)
I0711 11:32:32.493857 1275909 certs.go:484] found cert: /root/.minikube/certs/ca.pem (1074 bytes)
I0711 11:32:32.493866 1275909 certs.go:484] found cert: /root/.minikube/certs/cert.pem (1115 bytes)
I0711 11:32:32.493876 1275909 certs.go:484] found cert: /root/.minikube/certs/key.pem (1679 bytes)
I0711 11:32:32.494237 1275909 exec_runner.go:151] cp: /root/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0711 11:32:32.494295 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube1480889341 /var/lib/minikube/certs/ca.crt
I0711 11:32:32.504183 1275909 exec_runner.go:151] cp: /root/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0711 11:32:32.504275 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube903323424 /var/lib/minikube/certs/ca.key
I0711 11:32:32.514556 1275909 exec_runner.go:151] cp: /root/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0711 11:32:32.514644 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube1391683083 /var/lib/minikube/certs/proxy-client-ca.crt
I0711 11:32:32.524506 1275909 exec_runner.go:151] cp: /root/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0711 11:32:32.524585 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube3147105657 /var/lib/minikube/certs/proxy-client-ca.key
I0711 11:32:32.533346 1275909 exec_runner.go:151] cp: /root/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0711 11:32:32.534353 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube635127380 /var/lib/minikube/certs/apiserver.crt
I0711 11:32:32.544260 1275909 exec_runner.go:151] cp: /root/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0711 11:32:32.544517 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube4189631545 /var/lib/minikube/certs/apiserver.key
I0711 11:32:32.553541 1275909 exec_runner.go:151] cp: /root/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0711 11:32:32.553622 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube3299209458 /var/lib/minikube/certs/proxy-client.crt
I0711 11:32:32.563965 1275909 exec_runner.go:151] cp: /root/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0711 11:32:32.564044 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube3659271335 /var/lib/minikube/certs/proxy-client.key
I0711 11:32:32.573229 1275909 exec_runner.go:144] found /usr/share/ca-certificates/minikubeCA.pem, removing ...
I0711 11:32:32.573235 1275909 exec_runner.go:203] rm: /usr/share/ca-certificates/minikubeCA.pem
I0711 11:32:32.573253 1275909 exec_runner.go:151] cp: /root/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0711 11:32:32.573331 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube3496817235 /usr/share/ca-certificates/minikubeCA.pem
I0711 11:32:32.582850 1275909 exec_runner.go:151] cp: memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0711 11:32:32.582935 1275909 exec_runner.go:51] Run: sudo cp -a /tmp/minikube3770472195 /var/lib/minikube/kubeconfig
I0711 11:32:32.592094 1275909 exec_runner.go:51] Run: openssl version
I0711 11:32:32.594247 1275909 exec_runner.go:51] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0711 11:32:32.603596 1275909 exec_runner.go:51] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0711 11:32:32.604784 1275909 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul 11 11:32 /usr/share/ca-certificates/minikubeCA.pem
I0711 11:32:32.604801 1275909 exec_runner.go:51] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0711 11:32:32.612674 1275909 exec_runner.go:51] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0711 11:32:32.621844 1275909 exec_runner.go:51] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0711 11:32:32.622800 1275909 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: exit status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0711 11:32:32.622817 1275909 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:6000 CPUs:2 DiskSize:20000 Driver:none HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:false EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:10.190.181.134 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0711 11:32:32.622864 1275909 exec_runner.go:51] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0711 11:32:32.636522 1275909 exec_runner.go:51] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0711 11:32:32.645534 1275909 exec_runner.go:51] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0711 11:32:32.654496 1275909 exec_runner.go:51] Run: docker version --format {{.Server.Version}}
I0711 11:32:32.670076 1275909 exec_runner.go:51] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0711 11:32:32.679453 1275909 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: exit status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0711 11:32:32.679460 1275909 kubeadm.go:157] found existing configuration files:

I0711 11:32:32.679484 1275909 exec_runner.go:51] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0711 11:32:32.687485 1275909 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: exit status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0711 11:32:32.687511 1275909 exec_runner.go:51] Run: sudo rm -f /etc/kubernetes/admin.conf
I0711 11:32:32.695670 1275909 exec_runner.go:51] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0711 11:32:32.703847 1275909 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: exit status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0711 11:32:32.703872 1275909 exec_runner.go:51] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0711 11:32:32.712567 1275909 exec_runner.go:51] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0711 11:32:32.721492 1275909 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: exit status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0711 11:32:32.721518 1275909 exec_runner.go:51] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0711 11:32:32.730152 1275909 exec_runner.go:51] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0711 11:32:32.738505 1275909 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: exit status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0711 11:32:32.738529 1275909 exec_runner.go:51] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0711 11:32:32.747230 1275909 exec_runner.go:97] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0711 11:32:33.042740 1275909 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0711 11:32:33.042745 1275909 kubeadm.go:310] [preflight] Running pre-flight checks
I0711 11:32:33.092790 1275909 kubeadm.go:310] 	[WARNING HTTPProxy]: Connection to "https://10.190.181.134" uses proxy "http://proxy-dmz.intel.com:912". If that is not intended, adjust your proxy settings
I0711 11:32:33.092797 1275909 kubeadm.go:310] 	[WARNING HTTPProxyCIDR]: connection to "10.96.0.0/12" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
I0711 11:32:33.092801 1275909 kubeadm.go:310] 	[WARNING HTTPProxyCIDR]: connection to "10.244.0.0/16" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
I0711 11:32:33.161633 1275909 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0711 11:32:33.161638 1275909 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0711 11:32:33.161639 1275909 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0711 11:32:33.169303 1275909 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0711 11:32:33.170333 1275909 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0711 11:32:33.170363 1275909 kubeadm.go:310] [certs] Using existing ca certificate authority
I0711 11:32:33.170367 1275909 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0711 11:32:33.344842 1275909 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0711 11:32:33.536882 1275909 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0711 11:32:33.604179 1275909 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0711 11:32:33.772398 1275909 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0711 11:32:33.884805 1275909 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0711 11:32:33.884812 1275909 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost sr48dml033s0202] and IPs [10.190.181.134 127.0.0.1 ::1]
I0711 11:32:33.998445 1275909 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0711 11:32:33.998451 1275909 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost sr48dml033s0202] and IPs [10.190.181.134 127.0.0.1 ::1]
I0711 11:32:34.316974 1275909 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0711 11:32:34.366709 1275909 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0711 11:32:34.568130 1275909 kubeadm.go:310] [certs] Generating "sa" key and public key
I0711 11:32:34.568408 1275909 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0711 11:32:34.697853 1275909 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0711 11:32:34.729224 1275909 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0711 11:32:35.049584 1275909 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0711 11:32:35.139920 1275909 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0711 11:32:35.181003 1275909 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0711 11:32:35.181229 1275909 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0711 11:32:35.182478 1275909 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0711 11:32:35.184489 1275909 out.go:235]     ‚ñ™ Booting up control plane ...
I0711 11:32:35.184518 1275909 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0711 11:32:35.184530 1275909 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0711 11:32:35.184533 1275909 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0711 11:32:35.194524 1275909 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0711 11:32:35.200044 1275909 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0711 11:32:35.200050 1275909 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0711 11:32:35.478490 1275909 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0711 11:32:35.478495 1275909 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0711 11:32:35.980011 1275909 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.429759ms
I0711 11:32:35.981305 1275909 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0711 11:32:35.981309 1275909 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://10.190.181.134:8443/livez
I0711 11:32:35.981311 1275909 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0711 11:32:35.981314 1275909 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0711 11:32:37.055233 1275909 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 1.073780089s
I0711 11:32:37.484391 1275909 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 1.502850899s
I0711 11:36:35.982602 1275909 kubeadm.go:310] error execution phase wait-control-plane: failed while waiting for the control plane to start: kube-apiserver check failed at https://10.190.181.134:8443/livez: Get "https://control-plane.minikube.internal:8443/livez?timeout=10s": Gateway Timeout
I0711 11:36:35.982617 1275909 kubeadm.go:310] To see the stack trace of this error execute with --v=5 or higher
I0711 11:36:35.982629 1275909 kubeadm.go:310] [control-plane-check] kube-apiserver is not healthy after 4m0.001044075s
I0711 11:36:35.982630 1275909 kubeadm.go:310] 
I0711 11:36:35.982632 1275909 kubeadm.go:310] A control plane component may have crashed or exited when started by the container runtime.
I0711 11:36:35.982633 1275909 kubeadm.go:310] To troubleshoot, list all containers using your preferred container runtimes CLI.
I0711 11:36:35.982634 1275909 kubeadm.go:310] Here is one example how you may list all running Kubernetes containers by using crictl:
I0711 11:36:35.982636 1275909 kubeadm.go:310] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
I0711 11:36:35.982637 1275909 kubeadm.go:310] 	Once you have found the failing container, you can inspect its logs with:
I0711 11:36:35.982639 1275909 kubeadm.go:310] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'
I0711 11:36:35.982640 1275909 kubeadm.go:310] 
W0711 11:36:35.986824 1275909 out.go:270] üí¢  initialization failed, will try again: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": exit status 1
stdout:
[init] Using Kubernetes version: v1.33.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost sr48dml033s0202] and IPs [10.190.181.134 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost sr48dml033s0202] and IPs [10.190.181.134 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.429759ms
[control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
[control-plane-check] Checking kube-apiserver at https://10.190.181.134:8443/livez
[control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
[control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
[control-plane-check] kube-scheduler is healthy after 1.073780089s
[control-plane-check] kube-controller-manager is healthy after 1.502850899s
[control-plane-check] kube-apiserver is not healthy after 4m0.001044075s

A control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'


stderr:
	[WARNING HTTPProxy]: Connection to "https://10.190.181.134" uses proxy "http://proxy-dmz.intel.com:912". If that is not intended, adjust your proxy settings
	[WARNING HTTPProxyCIDR]: connection to "10.96.0.0/12" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
	[WARNING HTTPProxyCIDR]: connection to "10.244.0.0/16" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
error execution phase wait-control-plane: failed while waiting for the control plane to start: kube-apiserver check failed at https://10.190.181.134:8443/livez: Get "https://control-plane.minikube.internal:8443/livez?timeout=10s": Gateway Timeout
To see the stack trace of this error execute with --v=5 or higher

I0711 11:36:35.986882 1275909 exec_runner.go:51] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0711 11:36:36.787892 1275909 exec_runner.go:51] Run: sudo systemctl is-active --quiet service kubelet
I0711 11:36:36.800517 1275909 exec_runner.go:51] Run: docker version --format {{.Server.Version}}
I0711 11:36:36.818320 1275909 exec_runner.go:51] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0711 11:36:36.827292 1275909 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: exit status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0711 11:36:36.827298 1275909 kubeadm.go:157] found existing configuration files:

I0711 11:36:36.827322 1275909 exec_runner.go:51] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0711 11:36:36.836451 1275909 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: exit status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0711 11:36:36.836475 1275909 exec_runner.go:51] Run: sudo rm -f /etc/kubernetes/admin.conf
I0711 11:36:36.844751 1275909 exec_runner.go:51] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0711 11:36:36.853738 1275909 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: exit status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0711 11:36:36.853767 1275909 exec_runner.go:51] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0711 11:36:36.862013 1275909 exec_runner.go:51] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0711 11:36:36.870699 1275909 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: exit status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0711 11:36:36.870723 1275909 exec_runner.go:51] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0711 11:36:36.879773 1275909 exec_runner.go:51] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0711 11:36:36.888216 1275909 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: exit status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0711 11:36:36.888243 1275909 exec_runner.go:51] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0711 11:36:36.896613 1275909 exec_runner.go:97] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0711 11:36:36.925768 1275909 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0711 11:36:36.925773 1275909 kubeadm.go:310] [preflight] Running pre-flight checks
I0711 11:36:36.941332 1275909 kubeadm.go:310] 	[WARNING HTTPProxy]: Connection to "https://10.190.181.134" uses proxy "http://proxy-dmz.intel.com:912". If that is not intended, adjust your proxy settings
I0711 11:36:36.941339 1275909 kubeadm.go:310] 	[WARNING HTTPProxyCIDR]: connection to "10.96.0.0/12" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
I0711 11:36:36.941341 1275909 kubeadm.go:310] 	[WARNING HTTPProxyCIDR]: connection to "10.244.0.0/16" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
I0711 11:36:37.003767 1275909 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0711 11:36:37.003772 1275909 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0711 11:36:37.003775 1275909 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0711 11:36:37.007894 1275909 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0711 11:36:37.009696 1275909 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0711 11:36:37.009718 1275909 kubeadm.go:310] [certs] Using existing ca certificate authority
I0711 11:36:37.009723 1275909 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0711 11:36:37.009725 1275909 kubeadm.go:310] [certs] Using existing apiserver-kubelet-client certificate and key on disk
I0711 11:36:37.009726 1275909 kubeadm.go:310] [certs] Using existing front-proxy-ca certificate authority
I0711 11:36:37.009728 1275909 kubeadm.go:310] [certs] Using existing front-proxy-client certificate and key on disk
I0711 11:36:37.009729 1275909 kubeadm.go:310] [certs] Using existing etcd/ca certificate authority
I0711 11:36:37.009730 1275909 kubeadm.go:310] [certs] Using existing etcd/server certificate and key on disk
I0711 11:36:37.009731 1275909 kubeadm.go:310] [certs] Using existing etcd/peer certificate and key on disk
I0711 11:36:37.009732 1275909 kubeadm.go:310] [certs] Using existing etcd/healthcheck-client certificate and key on disk
I0711 11:36:37.009733 1275909 kubeadm.go:310] [certs] Using existing apiserver-etcd-client certificate and key on disk
I0711 11:36:37.009734 1275909 kubeadm.go:310] [certs] Using the existing "sa" key
I0711 11:36:37.009735 1275909 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0711 11:36:37.144383 1275909 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0711 11:36:37.238255 1275909 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0711 11:36:37.248188 1275909 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0711 11:36:37.450018 1275909 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0711 11:36:37.561446 1275909 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0711 11:36:37.561807 1275909 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0711 11:36:37.565238 1275909 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0711 11:36:37.567817 1275909 out.go:235]     ‚ñ™ Booting up control plane ...
I0711 11:36:37.567844 1275909 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0711 11:36:37.567856 1275909 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0711 11:36:37.567859 1275909 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0711 11:36:37.577032 1275909 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0711 11:36:37.581701 1275909 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0711 11:36:37.581707 1275909 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0711 11:36:37.842133 1275909 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0711 11:36:37.842139 1275909 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0711 11:36:38.343683 1275909 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.572086ms
I0711 11:36:38.345259 1275909 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0711 11:36:38.345265 1275909 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://10.190.181.134:8443/livez
I0711 11:36:38.345267 1275909 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0711 11:36:38.345268 1275909 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0711 11:36:39.481165 1275909 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 1.135710101s
I0711 11:36:39.512191 1275909 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 1.16683215s
I0711 11:40:38.345506 1275909 kubeadm.go:310] error execution phase wait-control-plane: failed while waiting for the control plane to start: kube-apiserver check failed at https://10.190.181.134:8443/livez: Get "https://control-plane.minikube.internal:8443/livez?timeout=10s": Gateway Timeout
I0711 11:40:38.345521 1275909 kubeadm.go:310] To see the stack trace of this error execute with --v=5 or higher
I0711 11:40:38.345532 1275909 kubeadm.go:310] [control-plane-check] kube-apiserver is not healthy after 4m0.000089964s
I0711 11:40:38.345534 1275909 kubeadm.go:310] 
I0711 11:40:38.345535 1275909 kubeadm.go:310] A control plane component may have crashed or exited when started by the container runtime.
I0711 11:40:38.345537 1275909 kubeadm.go:310] To troubleshoot, list all containers using your preferred container runtimes CLI.
I0711 11:40:38.345538 1275909 kubeadm.go:310] Here is one example how you may list all running Kubernetes containers by using crictl:
I0711 11:40:38.345540 1275909 kubeadm.go:310] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
I0711 11:40:38.345541 1275909 kubeadm.go:310] 	Once you have found the failing container, you can inspect its logs with:
I0711 11:40:38.345542 1275909 kubeadm.go:310] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'
I0711 11:40:38.345543 1275909 kubeadm.go:310] 
I0711 11:40:38.349522 1275909 kubeadm.go:394] duration metric: took 8m5.726703097s to StartCluster
I0711 11:40:38.349563 1275909 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0711 11:40:38.349605 1275909 exec_runner.go:51] Run: sudo crictl ps -a --quiet --name=kube-apiserver
E0711 11:40:38.356001 1275909 logs.go:279] Failed to list containers for "kube-apiserver": crictl list: sudo crictl ps -a --quiet --name=kube-apiserver: exit status 1
stdout:

stderr:
sudo: crictl: command not found
I0711 11:40:38.356015 1275909 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0711 11:40:38.356034 1275909 exec_runner.go:51] Run: sudo crictl ps -a --quiet --name=etcd
E0711 11:40:38.362258 1275909 logs.go:279] Failed to list containers for "etcd": crictl list: sudo crictl ps -a --quiet --name=etcd: exit status 1
stdout:

stderr:
sudo: crictl: command not found
I0711 11:40:38.362275 1275909 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0711 11:40:38.362304 1275909 exec_runner.go:51] Run: sudo crictl ps -a --quiet --name=coredns
E0711 11:40:38.368138 1275909 logs.go:279] Failed to list containers for "coredns": crictl list: sudo crictl ps -a --quiet --name=coredns: exit status 1
stdout:

stderr:
sudo: crictl: command not found
I0711 11:40:38.368155 1275909 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0711 11:40:38.368187 1275909 exec_runner.go:51] Run: sudo crictl ps -a --quiet --name=kube-scheduler
E0711 11:40:38.374177 1275909 logs.go:279] Failed to list containers for "kube-scheduler": crictl list: sudo crictl ps -a --quiet --name=kube-scheduler: exit status 1
stdout:

stderr:
sudo: crictl: command not found
I0711 11:40:38.374194 1275909 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0711 11:40:38.374223 1275909 exec_runner.go:51] Run: sudo crictl ps -a --quiet --name=kube-proxy
E0711 11:40:38.380185 1275909 logs.go:279] Failed to list containers for "kube-proxy": crictl list: sudo crictl ps -a --quiet --name=kube-proxy: exit status 1
stdout:

stderr:
sudo: crictl: command not found
I0711 11:40:38.380202 1275909 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0711 11:40:38.380232 1275909 exec_runner.go:51] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
E0711 11:40:38.385988 1275909 logs.go:279] Failed to list containers for "kube-controller-manager": crictl list: sudo crictl ps -a --quiet --name=kube-controller-manager: exit status 1
stdout:

stderr:
sudo: crictl: command not found
I0711 11:40:38.386005 1275909 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0711 11:40:38.386036 1275909 exec_runner.go:51] Run: sudo crictl ps -a --quiet --name=kindnet
E0711 11:40:38.391968 1275909 logs.go:279] Failed to list containers for "kindnet": crictl list: sudo crictl ps -a --quiet --name=kindnet: exit status 1
stdout:

stderr:
sudo: crictl: command not found
I0711 11:40:38.391989 1275909 logs.go:123] Gathering logs for kubelet ...
I0711 11:40:38.391995 1275909 exec_runner.go:51] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0711 11:40:38.437666 1275909 logs.go:123] Gathering logs for dmesg ...
I0711 11:40:38.437674 1275909 exec_runner.go:51] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0711 11:40:38.453135 1275909 logs.go:123] Gathering logs for describe nodes ...
I0711 11:40:38.453143 1275909 exec_runner.go:51] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0711 11:40:38.876053 1275909 logs.go:123] Gathering logs for Docker ...
I0711 11:40:38.876064 1275909 exec_runner.go:51] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0711 11:40:38.910756 1275909 logs.go:123] Gathering logs for container status ...
I0711 11:40:38.910763 1275909 exec_runner.go:51] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
W0711 11:40:38.941742 1275909 out.go:418] Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": exit status 1
stdout:
[init] Using Kubernetes version: v1.33.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.572086ms
[control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
[control-plane-check] Checking kube-apiserver at https://10.190.181.134:8443/livez
[control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
[control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
[control-plane-check] kube-controller-manager is healthy after 1.135710101s
[control-plane-check] kube-scheduler is healthy after 1.16683215s
[control-plane-check] kube-apiserver is not healthy after 4m0.000089964s

A control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'


stderr:
	[WARNING HTTPProxy]: Connection to "https://10.190.181.134" uses proxy "http://proxy-dmz.intel.com:912". If that is not intended, adjust your proxy settings
	[WARNING HTTPProxyCIDR]: connection to "10.96.0.0/12" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
	[WARNING HTTPProxyCIDR]: connection to "10.244.0.0/16" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
error execution phase wait-control-plane: failed while waiting for the control plane to start: kube-apiserver check failed at https://10.190.181.134:8443/livez: Get "https://control-plane.minikube.internal:8443/livez?timeout=10s": Gateway Timeout
To see the stack trace of this error execute with --v=5 or higher
W0711 11:40:38.941758 1275909 out.go:270] 
W0711 11:40:38.941803 1275909 out.go:270] üí£  Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": exit status 1
stdout:
[init] Using Kubernetes version: v1.33.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.572086ms
[control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
[control-plane-check] Checking kube-apiserver at https://10.190.181.134:8443/livez
[control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
[control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
[control-plane-check] kube-controller-manager is healthy after 1.135710101s
[control-plane-check] kube-scheduler is healthy after 1.16683215s
[control-plane-check] kube-apiserver is not healthy after 4m0.000089964s

A control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'


stderr:
	[WARNING HTTPProxy]: Connection to "https://10.190.181.134" uses proxy "http://proxy-dmz.intel.com:912". If that is not intended, adjust your proxy settings
	[WARNING HTTPProxyCIDR]: connection to "10.96.0.0/12" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
	[WARNING HTTPProxyCIDR]: connection to "10.244.0.0/16" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
error execution phase wait-control-plane: failed while waiting for the control plane to start: kube-apiserver check failed at https://10.190.181.134:8443/livez: Get "https://control-plane.minikube.internal:8443/livez?timeout=10s": Gateway Timeout
To see the stack trace of this error execute with --v=5 or higher

W0711 11:40:38.941838 1275909 out.go:270] 
W0711 11:40:38.942817 1275909 out.go:293] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0711 11:40:38.945195 1275909 out.go:201] 
W0711 11:40:38.947288 1275909 out.go:270] ‚ùå  Exiting due to GUEST_START: failed to start node: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": exit status 1
stdout:
[init] Using Kubernetes version: v1.33.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.572086ms
[control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
[control-plane-check] Checking kube-apiserver at https://10.190.181.134:8443/livez
[control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
[control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
[control-plane-check] kube-controller-manager is healthy after 1.135710101s
[control-plane-check] kube-scheduler is healthy after 1.16683215s
[control-plane-check] kube-apiserver is not healthy after 4m0.000089964s

A control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'


stderr:
	[WARNING HTTPProxy]: Connection to "https://10.190.181.134" uses proxy "http://proxy-dmz.intel.com:912". If that is not intended, adjust your proxy settings
	[WARNING HTTPProxyCIDR]: connection to "10.96.0.0/12" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
	[WARNING HTTPProxyCIDR]: connection to "10.244.0.0/16" uses proxy "http://proxy-dmz.intel.com:912". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
error execution phase wait-control-plane: failed while waiting for the control plane to start: kube-apiserver check failed at https://10.190.181.134:8443/livez: Get "https://control-plane.minikube.internal:8443/livez?timeout=10s": Gateway Timeout
To see the stack trace of this error execute with --v=5 or higher

W0711 11:40:38.947322 1275909 out.go:270] 
W0711 11:40:38.948301 1275909 out.go:293] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0711 11:40:38.950708 1275909 out.go:201] 


==> Docker <==
Jul 11 11:32:30 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:32:30.013912223-04:00" level=info msg="Loading containers: done."
Jul 11 11:32:30 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:32:30.019046648-04:00" level=info msg="Docker daemon" commit=e77ff99 containerd-snapshotter=false storage-driver=overlay2 version=28.3.2
Jul 11 11:32:30 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:32:30.019070927-04:00" level=info msg="Initializing buildkit"
Jul 11 11:32:30 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:32:30.037254204-04:00" level=info msg="Completed buildkit initialization"
Jul 11 11:32:30 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:32:30.042218025-04:00" level=info msg="Daemon has completed initialization"
Jul 11 11:32:30 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:32:30.042262802-04:00" level=info msg="API listen on /run/docker.sock"
Jul 11 11:32:30 SR48DML033S0202 systemd[1]: Started Docker Application Container Engine.
Jul 11 11:32:30 SR48DML033S0202 systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Jul 11 11:32:30 SR48DML033S0202 systemd[1]: cri-docker.service: Deactivated successfully.
Jul 11 11:32:30 SR48DML033S0202 systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Jul 11 11:32:31 SR48DML033S0202 systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Starting cri-dockerd dev (HEAD)"
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Start docker client with request timeout 0s"
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Hairpin mode is set to hairpin-veth"
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Loaded network plugin cni"
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Docker cri networking managed by network plugin cni"
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Setting cgroupDriver systemd"
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jul 11 11:32:31 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:31-04:00" level=info msg="Start cri-dockerd grpc backend"
Jul 11 11:32:31 SR48DML033S0202 systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jul 11 11:32:36 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:36-04:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0cb2342f89c7848a5c688f417b46037fb2a2d297742814300e43537f8ee955aa/resolv.conf as [nameserver 10.248.2.1 nameserver 10.109.90.215 nameserver 10.108.224.23 search iind.intel.com]"
Jul 11 11:32:36 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:36-04:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/648da779a868f2342172b7093b3704c6b4bf070982fd7a729d2c8093ac818e12/resolv.conf as [nameserver 10.248.2.1 nameserver 10.109.90.215 nameserver 10.108.224.23 search iind.intel.com]"
Jul 11 11:32:36 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:36-04:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/af7cb80f7494282f358e08938222a9db982abeb8982332646b9385fd3ac3145c/resolv.conf as [nameserver 10.248.2.1 nameserver 10.109.90.215 nameserver 10.108.224.23 search iind.intel.com]"
Jul 11 11:32:36 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:32:36-04:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3606649036fdec33b633d53b2e11eaa372043082f12b62d21eb980e9558c2399/resolv.conf as [nameserver 10.248.2.1 nameserver 10.109.90.215 nameserver 10.108.224.23 search iind.intel.com]"
Jul 11 11:32:47 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:32:47.172278440-04:00" level=info msg="ignoring event" container=2e0c779de116e92e92fb8d5cbff0ba3098562588c38724cb2129308d00ac9afa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:32:58 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:32:58.815084127-04:00" level=info msg="ignoring event" container=8a14129b7a027a6101bcf022c42d26a44b6a8f4d67b046b8be7025ab1e84c02e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:33:29 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:33:29.764113702-04:00" level=info msg="ignoring event" container=c50844529b03b1e5592c2068b705fc7367123f6dea659895fd53b9d2c141f83f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:34:01 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:34:01.755012943-04:00" level=info msg="ignoring event" container=09e14525b7a9c62c5efe67341304804aef4716995ac0427222223bed3be4b482 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:34:57 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:34:57.800007700-04:00" level=info msg="ignoring event" container=c6b2cbc9efbffea9fad7474b633d07cc4dd3d9ecf43bd9530f7e6080dab8e779 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:36:36 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:36:36.428175477-04:00" level=info msg="ignoring event" container=3606649036fdec33b633d53b2e11eaa372043082f12b62d21eb980e9558c2399 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:36:36 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:36:36.503102690-04:00" level=info msg="ignoring event" container=e729f09858cad5a65a4b5f92ca4fbcd5c1959d0404bdda23cdc6022fc9af5b91 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:36:36 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:36:36.541051256-04:00" level=info msg="ignoring event" container=0cb2342f89c7848a5c688f417b46037fb2a2d297742814300e43537f8ee955aa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:36:36 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:36:36.590766983-04:00" level=info msg="ignoring event" container=d96c20d03013c75e71c9b8278b6971db3cbb8e06ac014e3e7e85a15a4f634b04 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:36:36 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:36:36.621149272-04:00" level=info msg="ignoring event" container=af7cb80f7494282f358e08938222a9db982abeb8982332646b9385fd3ac3145c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:36:36 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:36:36.654194504-04:00" level=info msg="ignoring event" container=5213b91cbdfaaaf1702eb1f0eb893e2a50f7032055c3bf0c31b0383cc33839f8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:36:36 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:36:36.688743549-04:00" level=info msg="ignoring event" container=648da779a868f2342172b7093b3704c6b4bf070982fd7a729d2c8093ac818e12 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:36:36 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:36:36.759823590-04:00" level=info msg="ignoring event" container=e46c651761575a41f025747d371e35fa403ed5e5d3aa51847d1deaffba80359b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:36:38 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:38-04:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e13c24b137d717c371c6e570fee3aba102f20b580eaebf5dd28af72a65435f45/resolv.conf as [nameserver 10.248.2.1 nameserver 10.109.90.215 nameserver 10.108.224.23 search iind.intel.com]"
Jul 11 11:36:38 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:38-04:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8dd83b15dd93555c7125eb46620d1122fb88b9e3bc91a7c0748880573ae6ca3d/resolv.conf as [nameserver 10.248.2.1 nameserver 10.109.90.215 nameserver 10.108.224.23 search iind.intel.com]"
Jul 11 11:36:38 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:38-04:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/42d9c85a14c11cbc35f5c8a988d4320af01b1d9bc4bafcd3be15a15006534a72/resolv.conf as [nameserver 10.248.2.1 nameserver 10.109.90.215 nameserver 10.108.224.23 search iind.intel.com]"
Jul 11 11:36:38 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:38-04:00" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cdf3423ca8d478f05d890b8716955673725b4a6f4f6f1d2bc1520d380643f11a/resolv.conf as [nameserver 10.248.2.1 nameserver 10.109.90.215 nameserver 10.108.224.23 search iind.intel.com]"
Jul 11 11:36:46 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:46-04:00" level=error msg="error getting RW layer size for container ID '5213b91cbdfaaaf1702eb1f0eb893e2a50f7032055c3bf0c31b0383cc33839f8': Error response from daemon: No such container: 5213b91cbdfaaaf1702eb1f0eb893e2a50f7032055c3bf0c31b0383cc33839f8"
Jul 11 11:36:46 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:46-04:00" level=error msg="Set backoffDuration to : 1m0s for container ID '5213b91cbdfaaaf1702eb1f0eb893e2a50f7032055c3bf0c31b0383cc33839f8'"
Jul 11 11:36:46 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:46-04:00" level=error msg="error getting RW layer size for container ID 'e729f09858cad5a65a4b5f92ca4fbcd5c1959d0404bdda23cdc6022fc9af5b91': Error response from daemon: No such container: e729f09858cad5a65a4b5f92ca4fbcd5c1959d0404bdda23cdc6022fc9af5b91"
Jul 11 11:36:46 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:46-04:00" level=error msg="Set backoffDuration to : 1m0s for container ID 'e729f09858cad5a65a4b5f92ca4fbcd5c1959d0404bdda23cdc6022fc9af5b91'"
Jul 11 11:36:46 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:46-04:00" level=error msg="error getting RW layer size for container ID 'd96c20d03013c75e71c9b8278b6971db3cbb8e06ac014e3e7e85a15a4f634b04': Error response from daemon: No such container: d96c20d03013c75e71c9b8278b6971db3cbb8e06ac014e3e7e85a15a4f634b04"
Jul 11 11:36:46 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:36:46-04:00" level=error msg="Set backoffDuration to : 1m0s for container ID 'd96c20d03013c75e71c9b8278b6971db3cbb8e06ac014e3e7e85a15a4f634b04'"
Jul 11 11:36:49 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:36:49.510418964-04:00" level=info msg="ignoring event" container=64d6871b1824d879922cf8ef2a7a8d7d4ac79322efb056cd12069fcd5b568c6f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:37:01 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:37:01.152229894-04:00" level=info msg="ignoring event" container=e161f05eda33144022b93a94ec6c9221d41976289adaaa81985ed5bf1cec94d0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:37:34 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:37:34.938828184-04:00" level=info msg="ignoring event" container=f700a285ab7066049c418bcf5bb8a6be811f5319d9909dca866dba0a4157b404 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:38:17 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:38:17.843864182-04:00" level=info msg="ignoring event" container=351f199f59b1764fe84942730de5af561467915f11c4683ee8a031b915ad9fa8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:38:28 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:38:28-04:00" level=error msg="error getting RW layer size for container ID 'f700a285ab7066049c418bcf5bb8a6be811f5319d9909dca866dba0a4157b404': Error response from daemon: No such container: f700a285ab7066049c418bcf5bb8a6be811f5319d9909dca866dba0a4157b404"
Jul 11 11:38:28 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:38:28-04:00" level=error msg="Set backoffDuration to : 1m0s for container ID 'f700a285ab7066049c418bcf5bb8a6be811f5319d9909dca866dba0a4157b404'"
Jul 11 11:39:09 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:39:09.880938514-04:00" level=info msg="ignoring event" container=e5285304ba93178d07d5014e754634f82fbc43f64b5ead28fee54bc3076cb4ee module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:40:51 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:40:51.967591817-04:00" level=info msg="ignoring event" container=2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:43:44 SR48DML033S0202 dockerd[1276128]: time="2025-07-11T11:43:44.728099044-04:00" level=info msg="ignoring event" container=7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 11 11:43:48 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:43:48-04:00" level=error msg="error getting RW layer size for container ID '2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18': Error response from daemon: No such container: 2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:43:48 SR48DML033S0202 cri-dockerd[1276509]: time="2025-07-11T11:43:48-04:00" level=error msg="Set backoffDuration to : 1m0s for container ID '2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18'"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
7c10fd321e468       ef43894fa110c       2 minutes ago       Exited              kube-controller-manager   6                   8dd83b15dd935       kube-controller-manager-sr48dml033s0202
e234e7eb6a8f5       398c985c0d950       9 minutes ago       Running             kube-scheduler            0                   cdf3423ca8d47       kube-scheduler-sr48dml033s0202
9d2f73ee329b1       c6ab243b29f82       9 minutes ago       Running             kube-apiserver            0                   42d9c85a14c11       kube-apiserver-sr48dml033s0202
3087a7d32ef9a       499038711c081       9 minutes ago       Running             etcd                      0                   e13c24b137d71       etcd-sr48dml033s0202


==> describe nodes <==
Name:               sr48dml033s0202
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=sr48dml033s0202
                    kubernetes.io/os=linux
Annotations:        volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 11 Jul 2025 11:36:40 -0400
Taints:             node.kubernetes.io/not-ready:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  sr48dml033s0202
  AcquireTime:     <unset>
  RenewTime:       Fri, 11 Jul 2025 11:45:51 -0400
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 11 Jul 2025 11:41:46 -0400   Fri, 11 Jul 2025 11:36:38 -0400   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 11 Jul 2025 11:41:46 -0400   Fri, 11 Jul 2025 11:36:38 -0400   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 11 Jul 2025 11:41:46 -0400   Fri, 11 Jul 2025 11:36:38 -0400   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 11 Jul 2025 11:41:46 -0400   Fri, 11 Jul 2025 11:36:40 -0400   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  10.190.181.134
  Hostname:    sr48dml033s0202
Capacity:
  cpu:                128
  ephemeral-storage:  71616Mi
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             131078076Ki
  pods:               110
Allocatable:
  cpu:                128
  ephemeral-storage:  71616Mi
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             131078076Ki
  pods:               110
System Info:
  Machine ID:                 a8f6888a717645338cb677af67cbef2c
  System UUID:                88888888-8887-0c18-0706-0722a5a5a5a5
  Boot ID:                    df09a607-c092-4e03-b735-9bfcb06b484a
  Kernel Version:             6.2.0-emr.bkc.6.2.19.2.52.x86_64
  OS Image:                   CentOS Stream 9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.3.2
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
Non-terminated Pods:          (4 in total)
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                       ------------  ----------  ---------------  -------------  ---
  kube-system                 etcd-sr48dml033s0202                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         9m11s
  kube-system                 kube-apiserver-sr48dml033s0202             250m (0%)     0 (0%)      0 (0%)           0 (0%)         9m8s
  kube-system                 kube-controller-manager-sr48dml033s0202    200m (0%)     0 (0%)      0 (0%)           0 (0%)         9m5s
  kube-system                 kube-scheduler-sr48dml033s0202             100m (0%)     0 (0%)      0 (0%)           0 (0%)         9m9s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                650m (0%)   0 (0%)
  memory             100Mi (0%)  0 (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                    From     Message
  ----    ------                   ----                   ----     -------
  Normal  Starting                 9m14s                  kubelet  Starting kubelet.
  Normal  NodeHasSufficientMemory  9m14s (x8 over 9m14s)  kubelet  Node sr48dml033s0202 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    9m14s (x8 over 9m14s)  kubelet  Node sr48dml033s0202 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     9m14s (x7 over 9m14s)  kubelet  Node sr48dml033s0202 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  9m14s                  kubelet  Updated Node Allocatable limit across pods


==> dmesg <==
[  +0.065920]  #118
[  +0.067409]  #119
[  +0.068610]  #120
[  +0.065989]  #121
[  +0.068002]  #122
[  +0.066409]  #123
[  +0.068880]  #124
[  +0.069713]  #125
[  +0.067063]  #126
[  +0.069938]  #127

[  +0.079480] *************************************************************
[  +0.000002] **     NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE    **
[  +0.000000] **                                                         **
[  +0.000001] **  IOMMU DebugFS SUPPORT HAS BEEN ENABLED IN THIS KERNEL  **
[  +0.000001] **                                                         **
[  +0.000001] ** This means that this kernel is built to expose internal **
[  +0.000001] ** IOMMU data structures, which may compromise security on **
[  +0.000001] ** your system.                                            **
[  +0.000000] **                                                         **
[  +0.000001] ** If you see this message and you are not debugging the   **
[  +0.000001] ** kernel, report this immediately to your vendor!         **
[  +0.000000] **                                                         **
[  +0.000001] **     NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE    **
[  +0.000001] *************************************************************
[  +0.006190] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
[  +2.276152] Block PNP0C02 PFRU device  SSRU
[  +0.000000] Block PNP0C02 PFRU device  SSTS
[  +0.435688] fail to initialize ptp_kvm
[  +0.012629] usb: port power management may be unreliable
[  +0.192525] pstore: backend 'erst' already in use: ignoring 'efi_pstore'
[  +0.179797] systemd[1]: /usr/lib/systemd/system/systemd-udevd.service:45: Unknown key name 'LimitNOFILESoft' in section 'Service', ignoring.
[  +2.115574] ipmi_si IPI0001:00: The BMC does not support clearing the recv irq bit, compensating, but the BMC needs to be fixed.
[  +0.072272] power_meter ACPI000D:00: Ignoring unsafe software power cap!
[  +0.001696] power_meter ACPI000D:00: hwmon_device_register() is deprecated. Please convert the driver to use hwmon_device_register_with_info().
[  +0.005449] idxd_probe: 561
[  +0.000480] i2c i2c-2: Systems with more than 4 memory slots not supported yet, not instantiating SPD
[  +0.197799] idxd_probe: 561
[  +0.004589] idxd_probe: 561
[  +0.019071] idxd_probe: 561
[  +0.001327] tdx: initialization failed: TDX is disabled.
[  +0.015497] idxd_probe: 561
[  +0.640766] idxd_probe: 561
[  +0.036862] idxd_probe: 561
[  +0.003938] idxd_probe: 561
[  +1.006492] microcode: Current microcode rev 0xa1000200 greater than 0x21000230, aborting
[  +5.891402] pax: loading out-of-tree module taints kernel.
[  +2.214338] sep5_50: Driver loading... sym_lookup_func_addr=ffffffff81249c40
[  +0.026734] sep5_50: [warning] [lwpmudrv_Detect_PMT_Endpoints@6695]: Address of Intel(R) PMT function is invalid

[Jul10 02:50] node: epoll_ctl support in io_uring is deprecated and will be removed in a future Linux kernel version.
[Jul10 05:03] node: epoll_ctl support in io_uring is deprecated and will be removed in a future Linux kernel version.
[Jul10 05:50] tmpfs: Unknown parameter 'noswap'
[  +4.069718] tmpfs: Unknown parameter 'noswap'
[Jul10 05:54] tmpfs: Unknown parameter 'noswap'
[  +3.877864] tmpfs: Unknown parameter 'noswap'
[Jul10 05:57] tmpfs: Unknown parameter 'noswap'
[  +3.831321] tmpfs: Unknown parameter 'noswap'
[Jul10 06:10] tmpfs: Unknown parameter 'noswap'
[Jul10 06:14] tmpfs: Unknown parameter 'noswap'


==> etcd [3087a7d32ef9] <==
{"level":"warn","ts":"2025-07-11T15:36:38.843248Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-07-11T15:36:38.843313Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-07-11T15:36:38.843322Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://10.190.181.134:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://10.190.181.134:2380","--initial-cluster=sr48dml033s0202=https://10.190.181.134:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://10.190.181.134:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://10.190.181.134:2380","--name=sr48dml033s0202","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-07-11T15:36:38.843362Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-07-11T15:36:38.843369Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://10.190.181.134:2380"]}
{"level":"info","ts":"2025-07-11T15:36:38.843392Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-11T15:36:38.843693Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://10.190.181.134:2379","https://127.0.0.1:2379"]}
{"level":"info","ts":"2025-07-11T15:36:38.843741Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":128,"max-cpu-available":128,"member-initialized":false,"name":"sr48dml033s0202","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://10.190.181.134:2380"],"listen-peer-urls":["https://10.190.181.134:2380"],"advertise-client-urls":["https://10.190.181.134:2379"],"listen-client-urls":["https://10.190.181.134:2379","https://127.0.0.1:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"sr48dml033s0202=https://10.190.181.134:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-07-11T15:36:38.845290Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.437983ms"}
{"level":"info","ts":"2025-07-11T15:36:38.846545Z","caller":"etcdserver/raft.go:506","msg":"starting local member","local-member-id":"69d2386342aa2dbd","cluster-id":"13e679ea1a8ec1e8"}
{"level":"info","ts":"2025-07-11T15:36:38.846588Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd switched to configuration voters=()"}
{"level":"info","ts":"2025-07-11T15:36:38.846609Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd became follower at term 0"}
{"level":"info","ts":"2025-07-11T15:36:38.846616Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 69d2386342aa2dbd [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-07-11T15:36:38.846623Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd became follower at term 1"}
{"level":"info","ts":"2025-07-11T15:36:38.846647Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd switched to configuration voters=(7625219118063037885)"}
{"level":"warn","ts":"2025-07-11T15:36:38.847536Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-07-11T15:36:38.848808Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-07-11T15:36:38.848871Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":0}
{"level":"info","ts":"2025-07-11T15:36:38.849473Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-07-11T15:36:38.849983Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"69d2386342aa2dbd","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-07-11T15:36:38.850183Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-11T15:36:38.850312Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"69d2386342aa2dbd","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-07-11T15:36:38.850328Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-11T15:36:38.850370Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-11T15:36:38.850389Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-11T15:36:38.850457Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd switched to configuration voters=(7625219118063037885)"}
{"level":"info","ts":"2025-07-11T15:36:38.850521Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"13e679ea1a8ec1e8","local-member-id":"69d2386342aa2dbd","added-peer-id":"69d2386342aa2dbd","added-peer-peer-urls":["https://10.190.181.134:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-07-11T15:36:38.851091Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-11T15:36:38.851122Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"10.190.181.134:2380"}
{"level":"info","ts":"2025-07-11T15:36:38.851145Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"10.190.181.134:2380"}
{"level":"info","ts":"2025-07-11T15:36:38.851297Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"69d2386342aa2dbd","initial-advertise-peer-urls":["https://10.190.181.134:2380"],"listen-peer-urls":["https://10.190.181.134:2380"],"advertise-client-urls":["https://10.190.181.134:2379"],"listen-client-urls":["https://10.190.181.134:2379","https://127.0.0.1:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-07-11T15:36:38.851326Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-07-11T15:36:39.647486Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd is starting a new election at term 1"}
{"level":"info","ts":"2025-07-11T15:36:39.647505Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd became pre-candidate at term 1"}
{"level":"info","ts":"2025-07-11T15:36:39.647513Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd received MsgPreVoteResp from 69d2386342aa2dbd at term 1"}
{"level":"info","ts":"2025-07-11T15:36:39.647528Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd became candidate at term 2"}
{"level":"info","ts":"2025-07-11T15:36:39.647546Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd received MsgVoteResp from 69d2386342aa2dbd at term 2"}
{"level":"info","ts":"2025-07-11T15:36:39.647551Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"69d2386342aa2dbd became leader at term 2"}
{"level":"info","ts":"2025-07-11T15:36:39.647555Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 69d2386342aa2dbd elected leader 69d2386342aa2dbd at term 2"}
{"level":"info","ts":"2025-07-11T15:36:39.648426Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-11T15:36:39.649528Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"69d2386342aa2dbd","local-member-attributes":"{Name:sr48dml033s0202 ClientURLs:[https://10.190.181.134:2379]}","request-path":"/0/members/69d2386342aa2dbd/attributes","cluster-id":"13e679ea1a8ec1e8","publish-timeout":"7s"}
{"level":"info","ts":"2025-07-11T15:36:39.649548Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-11T15:36:39.649752Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-11T15:36:39.649960Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-11T15:36:39.650107Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-11T15:36:39.650189Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-07-11T15:36:39.650209Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-07-11T15:36:39.650209Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"13e679ea1a8ec1e8","local-member-id":"69d2386342aa2dbd","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-11T15:36:39.650288Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-11T15:36:39.650305Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-11T15:36:39.650435Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"10.190.181.134:2379"}
{"level":"info","ts":"2025-07-11T15:36:39.650516Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> kernel <==
 11:45:52 up 63 days,  5:14,  1 user,  load average: 0.01, 0.10, 0.08
Linux SR48DML033S0202 6.2.0-emr.bkc.6.2.19.2.52.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Nov  1 22:56:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="CentOS Stream 9"


==> kube-apiserver [9d2f73ee329b] <==
I0711 15:36:40.050710       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0711 15:36:40.050709       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0711 15:36:40.050744       1 controller.go:142] Starting OpenAPI controller
I0711 15:36:40.050757       1 controller.go:90] Starting OpenAPI V3 controller
I0711 15:36:40.050764       1 aggregator.go:169] waiting for initial CRD sync...
I0711 15:36:40.050764       1 naming_controller.go:299] Starting NamingConditionController
I0711 15:36:40.050769       1 establishing_controller.go:81] Starting EstablishingController
I0711 15:36:40.050772       1 controller.go:78] Starting OpenAPI AggregationController
I0711 15:36:40.050778       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0711 15:36:40.050785       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0711 15:36:40.050790       1 crd_finalizer.go:269] Starting CRDFinalizer
I0711 15:36:40.050766       1 controller.go:119] Starting legacy_token_tracking_controller
I0711 15:36:40.050807       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0711 15:36:40.050820       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0711 15:36:40.050838       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0711 15:36:40.050847       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0711 15:36:40.050786       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0711 15:36:40.050866       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0711 15:36:40.050899       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0711 15:36:40.051005       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0711 15:36:40.051009       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0711 15:36:40.051035       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0711 15:36:40.051038       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0711 15:36:40.050840       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0711 15:36:40.051431       1 shared_informer.go:350] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I0711 15:36:40.057884       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0711 15:36:40.057906       1 policy_source.go:240] refreshing policies
E0711 15:36:40.110348       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0711 15:36:40.151617       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0711 15:36:40.151662       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0711 15:36:40.151737       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0711 15:36:40.151758       1 default_servicecidr_controller.go:165] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0711 15:36:40.151869       1 cache.go:39] Caches are synced for LocalAvailability controller
I0711 15:36:40.151955       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0711 15:36:40.151959       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0711 15:36:40.152032       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0711 15:36:40.152045       1 aggregator.go:171] initial CRD sync complete...
I0711 15:36:40.152053       1 autoregister_controller.go:144] Starting autoregister controller
I0711 15:36:40.152056       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0711 15:36:40.152058       1 cache.go:39] Caches are synced for autoregister controller
I0711 15:36:40.152107       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0711 15:36:40.152111       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0711 15:36:40.152126       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0711 15:36:40.152206       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0711 15:36:40.152222       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0711 15:36:40.157875       1 controller.go:667] quota admission added evaluator for: namespaces
I0711 15:36:40.158478       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0711 15:36:40.158512       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0711 15:36:40.166110       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0711 15:36:40.166317       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0711 15:36:40.312327       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0711 15:36:41.053773       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0711 15:36:41.057160       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0711 15:36:41.057168       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0711 15:36:41.467667       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0711 15:36:41.500664       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0711 15:36:41.560103       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0711 15:36:41.565027       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [10.190.181.134]
I0711 15:36:41.565808       1 controller.go:667] quota admission added evaluator for: endpoints
I0711 15:36:41.569270       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io


==> kube-controller-manager [7c10fd321e46] <==
I0711 15:43:34.539351       1 serving.go:386] Generated self-signed cert in-memory
I0711 15:43:34.692953       1 controllermanager.go:188] "Starting" version="v1.33.1"
I0711 15:43:34.692961       1 controllermanager.go:190] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0711 15:43:34.694345       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0711 15:43:34.694402       1 secure_serving.go:211] Serving securely on 127.0.0.1:10257
I0711 15:43:34.694361       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0711 15:43:34.694529       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0711 15:43:44.711566       1 controllermanager.go:242] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: Get \"https://10.190.181.134:8443/healthz\": Forbidden"


==> kube-scheduler [e234e7eb6a8f] <==
E0711 15:43:07.183396       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://10.190.181.134:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0711 15:43:07.533159       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://10.190.181.134:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0711 15:43:13.634944       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://10.190.181.134:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0711 15:43:16.480009       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://10.190.181.134:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0711 15:43:17.778381       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0711 15:43:19.864979       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0711 15:43:23.509458       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://10.190.181.134:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0711 15:43:28.688282       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://10.190.181.134:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0711 15:43:29.515828       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://10.190.181.134:8443/api/v1/nodes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0711 15:43:30.310088       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://10.190.181.134:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0711 15:43:30.839165       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0711 15:43:33.345906       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://10.190.181.134:8443/api/v1/namespaces?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0711 15:43:34.147090       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0711 15:43:40.671351       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0711 15:43:41.502430       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://10.190.181.134:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0711 15:43:45.058190       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://10.190.181.134:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0711 15:43:48.119595       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://10.190.181.134:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0711 15:43:48.755269       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://10.190.181.134:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0711 15:43:50.273985       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://10.190.181.134:8443/api/v1/services?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0711 15:44:03.811991       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://10.190.181.134:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0711 15:44:04.321832       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://10.190.181.134:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0711 15:44:05.739780       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0711 15:44:11.397458       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0711 15:44:11.563222       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0711 15:44:15.544806       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://10.190.181.134:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0711 15:44:16.484098       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0711 15:44:20.238845       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://10.190.181.134:8443/api/v1/nodes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0711 15:44:21.343275       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://10.190.181.134:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0711 15:44:22.301363       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://10.190.181.134:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0711 15:44:26.114328       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://10.190.181.134:8443/api/v1/namespaces?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0711 15:44:34.600970       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0711 15:44:39.845695       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://10.190.181.134:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0711 15:44:41.984141       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0711 15:44:43.027846       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://10.190.181.134:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0711 15:44:43.512017       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://10.190.181.134:8443/api/v1/services?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0711 15:44:45.779475       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://10.190.181.134:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0711 15:44:49.850111       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://10.190.181.134:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0711 15:44:51.806166       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0711 15:44:51.990829       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://10.190.181.134:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0711 15:44:56.717900       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://10.190.181.134:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0711 15:44:57.749455       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://10.190.181.134:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0711 15:44:57.867087       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://10.190.181.134:8443/api/v1/nodes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0711 15:45:01.534418       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0711 15:45:04.958082       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0711 15:45:12.219653       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://10.190.181.134:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0711 15:45:18.856852       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://10.190.181.134:8443/api/v1/namespaces?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0711 15:45:18.956274       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://10.190.181.134:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0711 15:45:21.033444       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://10.190.181.134:8443/api/v1/services?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0711 15:45:22.652015       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://10.190.181.134:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0711 15:45:26.381899       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0711 15:45:26.601033       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0711 15:45:28.763181       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://10.190.181.134:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0711 15:45:29.642310       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://10.190.181.134:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0711 15:45:31.428186       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://10.190.181.134:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0711 15:45:31.495174       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0711 15:45:34.737840       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0711 15:45:39.284683       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://10.190.181.134:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0711 15:45:42.405397       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://10.190.181.134:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0711 15:45:43.287314       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://10.190.181.134:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0711 15:45:48.421526       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://10.190.181.134:8443/api/v1/nodes?limit=500&resourceVersion=0\": Forbidden" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"


==> kubelet <==
Jul 11 11:40:15 SR48DML033S0202 kubelet[1279545]: I0711 11:40:15.172802 1279545 scope.go:117] "RemoveContainer" containerID="e5285304ba93178d07d5014e754634f82fbc43f64b5ead28fee54bc3076cb4ee"
Jul 11 11:40:15 SR48DML033S0202 kubelet[1279545]: E0711 11:40:15.172886 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:40:28 SR48DML033S0202 kubelet[1279545]: I0711 11:40:28.172994 1279545 scope.go:117] "RemoveContainer" containerID="e5285304ba93178d07d5014e754634f82fbc43f64b5ead28fee54bc3076cb4ee"
Jul 11 11:40:28 SR48DML033S0202 kubelet[1279545]: E0711 11:40:28.173075 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:40:41 SR48DML033S0202 kubelet[1279545]: I0711 11:40:41.172699 1279545 scope.go:117] "RemoveContainer" containerID="e5285304ba93178d07d5014e754634f82fbc43f64b5ead28fee54bc3076cb4ee"
Jul 11 11:40:52 SR48DML033S0202 kubelet[1279545]: I0711 11:40:52.955452 1279545 scope.go:117] "RemoveContainer" containerID="e5285304ba93178d07d5014e754634f82fbc43f64b5ead28fee54bc3076cb4ee"
Jul 11 11:40:52 SR48DML033S0202 kubelet[1279545]: I0711 11:40:52.955627 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:40:52 SR48DML033S0202 kubelet[1279545]: E0711 11:40:52.955697 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:40:57 SR48DML033S0202 kubelet[1279545]: I0711 11:40:57.766683 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:40:57 SR48DML033S0202 kubelet[1279545]: E0711 11:40:57.766770 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:41:00 SR48DML033S0202 kubelet[1279545]: I0711 11:41:00.378830 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:41:00 SR48DML033S0202 kubelet[1279545]: E0711 11:41:00.378911 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:41:13 SR48DML033S0202 kubelet[1279545]: I0711 11:41:13.172943 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:41:13 SR48DML033S0202 kubelet[1279545]: E0711 11:41:13.173025 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:41:24 SR48DML033S0202 kubelet[1279545]: I0711 11:41:24.173472 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:41:24 SR48DML033S0202 kubelet[1279545]: E0711 11:41:24.173557 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:41:39 SR48DML033S0202 kubelet[1279545]: I0711 11:41:39.172679 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:41:39 SR48DML033S0202 kubelet[1279545]: E0711 11:41:39.172763 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:41:53 SR48DML033S0202 kubelet[1279545]: I0711 11:41:53.172594 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:41:53 SR48DML033S0202 kubelet[1279545]: E0711 11:41:53.172677 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:42:06 SR48DML033S0202 kubelet[1279545]: I0711 11:42:06.175571 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:42:06 SR48DML033S0202 kubelet[1279545]: E0711 11:42:06.175649 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:42:17 SR48DML033S0202 kubelet[1279545]: I0711 11:42:17.172344 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:42:17 SR48DML033S0202 kubelet[1279545]: E0711 11:42:17.172436 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:42:29 SR48DML033S0202 kubelet[1279545]: I0711 11:42:29.172438 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:42:29 SR48DML033S0202 kubelet[1279545]: E0711 11:42:29.172536 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:42:42 SR48DML033S0202 kubelet[1279545]: I0711 11:42:42.173117 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:42:42 SR48DML033S0202 kubelet[1279545]: E0711 11:42:42.173200 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:42:57 SR48DML033S0202 kubelet[1279545]: I0711 11:42:57.172333 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:42:57 SR48DML033S0202 kubelet[1279545]: E0711 11:42:57.172430 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:43:09 SR48DML033S0202 kubelet[1279545]: I0711 11:43:09.172932 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:43:09 SR48DML033S0202 kubelet[1279545]: E0711 11:43:09.173013 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:43:20 SR48DML033S0202 kubelet[1279545]: I0711 11:43:20.173270 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:43:20 SR48DML033S0202 kubelet[1279545]: E0711 11:43:20.173360 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:43:34 SR48DML033S0202 kubelet[1279545]: I0711 11:43:34.172771 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:43:45 SR48DML033S0202 kubelet[1279545]: I0711 11:43:45.428767 1279545 scope.go:117] "RemoveContainer" containerID="2a42e7a727df3c5932d600856b371f9e2299f8ef2dec78c5828ee8361a15ba18"
Jul 11 11:43:45 SR48DML033S0202 kubelet[1279545]: I0711 11:43:45.428936 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:43:45 SR48DML033S0202 kubelet[1279545]: E0711 11:43:45.429000 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:43:47 SR48DML033S0202 kubelet[1279545]: I0711 11:43:47.766955 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:43:47 SR48DML033S0202 kubelet[1279545]: E0711 11:43:47.767056 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:43:50 SR48DML033S0202 kubelet[1279545]: I0711 11:43:50.378900 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:43:50 SR48DML033S0202 kubelet[1279545]: E0711 11:43:50.378984 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:44:01 SR48DML033S0202 kubelet[1279545]: I0711 11:44:01.173140 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:44:01 SR48DML033S0202 kubelet[1279545]: E0711 11:44:01.173226 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:44:13 SR48DML033S0202 kubelet[1279545]: I0711 11:44:13.173345 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:44:13 SR48DML033S0202 kubelet[1279545]: E0711 11:44:13.173440 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:44:25 SR48DML033S0202 kubelet[1279545]: I0711 11:44:25.172733 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:44:25 SR48DML033S0202 kubelet[1279545]: E0711 11:44:25.172815 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:44:38 SR48DML033S0202 kubelet[1279545]: I0711 11:44:38.172365 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:44:38 SR48DML033S0202 kubelet[1279545]: E0711 11:44:38.172453 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:44:50 SR48DML033S0202 kubelet[1279545]: I0711 11:44:50.175474 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:44:50 SR48DML033S0202 kubelet[1279545]: E0711 11:44:50.175546 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:45:04 SR48DML033S0202 kubelet[1279545]: I0711 11:45:04.173496 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:45:04 SR48DML033S0202 kubelet[1279545]: E0711 11:45:04.173598 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:45:18 SR48DML033S0202 kubelet[1279545]: I0711 11:45:18.175751 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:45:18 SR48DML033S0202 kubelet[1279545]: E0711 11:45:18.177147 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:45:29 SR48DML033S0202 kubelet[1279545]: I0711 11:45:29.172330 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:45:29 SR48DML033S0202 kubelet[1279545]: E0711 11:45:29.172408 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"
Jul 11 11:45:42 SR48DML033S0202 kubelet[1279545]: I0711 11:45:42.173475 1279545 scope.go:117] "RemoveContainer" containerID="7c10fd321e468d779a76a770c2446f21b0bd0430ca0e06435b59210bd6773e18"
Jul 11 11:45:42 SR48DML033S0202 kubelet[1279545]: E0711 11:45:42.173563 1279545 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-sr48dml033s0202_kube-system(c824563d954c69488c047a60d65c6d04)\"" pod="kube-system/kube-controller-manager-sr48dml033s0202" podUID="c824563d954c69488c047a60d65c6d04"

